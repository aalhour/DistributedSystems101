#+Title: Distributed Systems 101
#+Author: @lvh
#+Email: _@lvh.io

#+OPTIONS: toc:nil reveal_rolling_links:nil num:nil reveal_history:true
#+REVEAL_TRANS: linear
#+REVEAL_THEME: lvh

* Introduction

** Who am I?

   #+ATTR_HTML: :style width:80%
   [[./media/lvh.svg]]

** Rackspace

   #+ATTR_HTML: :style width:90%
   [[./media/rackspace.svg]]

** AutoScale

   #+ATTR_HTML: :style width:90%
   [[./media/threeotters.jpg]]

** AutoScale

   #+ATTR_REVEAL: :frag roll-in
   * Distributed system
   * Manages distributed systems
   * Running on distributed systems
   * Interacting with distributed systems

** Why this talk?

   Parallels with Crypto 101

   #+ATTR_REVEAL: :frag roll-in
   * Field considered exclusive domain of experts
   * Abstinence-only education doesn't work
   * Lots of material, but not organized for self-teaching
   * We can't afford not to care

** Disconnect

    * Many theoreticians ignore practice
    * Many practitioners ignore theory

** Distributed Systems 101

   "Just enough" distributed systems

   #+ATTR_REVEAL: :frag roll-in
   * to whet your appetite
   * to shoot yourself in the foot

* Distributed systems and concurrency

** What /is/ a distributed system?

   A system is distributed if a machine I've never heard of can cause
   my program to fail.

   /Leslie Lamport/

** Paradox

   * Why do we use them? Reliability!
   * Experts' primary concern? Failure!

** Fundamental constraints

   1. Information travels at /c/
   2. Independent components fail independently

** Fallacies

   1. The network is reliable.
   2. Latency is zero.
   3. Bandwidth is infinite.
   4. The network is secure.
   5. Topology doesn't change.
   6. There is one administrator.
   7. Transport cost is zero.
   8. The network is homogeneous.

** Examples

   * Basically everything (e.g., your laptop)
     * Speed of light isn't infinite
     * RAM is /all the way over there/
   * Typically:
     * Any system with > 1 machine
     * Connected via network

** Concurrency

   Overlapping operations

** Consistency models

   #+ATTR_HTML: :style width:80%
   [[./media/ConsistencyModels.svg]]

   #+BEGIN_NOTES
   http://www.bailis.org/papers/hat-vldb2014.pdf
   http://www.ics.forth.gr/tech-reports/2013/2013.TR439_Survey_on_Consistency_Conditions.pdf
   http://www.allthingsdistributed.com/2008/12/eventually_consistent.html
   #+END_NOTES

** Serializability

   âˆƒ serial execution of transactions with the same result

** Linearizability

   All operations on a register look like they happened instantly

** Strong serializability

   Linearizable & serializable

** CAP
** Pick any two:

   * Consistency
   * Availability
   * Partition tolerance

** What does that even mean?

   * Availability, n.: all active nodes answer every query
   * Consistency:, n.: linearizability
   * Partition tolerance, p.: failure-resistant

** Pick any two

   #+ATTR_HTML: :style width:50%
   [[./media/cap-venn-base.svg]]

** Pick any two

   #+ATTR_HTML: :style width:50%
   [[./media/cap-venn-any2.svg]]

** Can't sacrifice partition tolerance

   * Partition tolerance is failure tolerance
   * Networks, nodes fail all the time
   * Latency happens; indistinguishable
   * P(no failures) < 1 - P(one node works)^N
     * Cascades, Hurst exponent

** Pick any two: AP or CP

   #+ATTR_HTML: :style width:50%
   [[./media/cap-venn-any2-withp.svg]]

** Examples

   * Zookeeper is CP: consistent ops that sometimes fail
   * Cassandra is AP: inconsistent ops that (usually) succeed

** Reality

   * CAP's C is linearizability
   * CAP's A is ops on /any/ node
   * These are very strong guarantees!

** Gradations

   #+ATTR_HTML: :style width:80%
   [[./media/ap-cp.svg]]

** Reality

   * There are /many/ consistency models (real & theoretical)
   * There are /many/ levels of availability (0-100%)
   * No reason to sacrifice either outside of failures
   * Systems can sacrifice many things during failures

** Example of creative sacrifice: ~etcd~

   * Normally: consistency all the way
   * Option of doing inconsistent reads
   * Maybe get some stale data
   * ... but still works if the cluster is on fire!

* Impossibility results

** FLP

** Wallclocks

* Components

  (High and low level)

** Queues

** Consensus protocols

** Examples

   * ZAB (used in Zookeeper)
   * Paxos (think 2PC, but distributed)
   * Raft (used in ~etcd~)

** Consensus protocol recipes

   * Set partitioning
   * Consistent counters
   * ...

** CRDTs

** Previously: CAP

    * Presented as trade-off: C versus A
    * Doesn't explain what we can do /on/ that line
    * CRDTs: wonderful modern example of what we can do

* Conclusion

** Yay, distributed systems!

   * More resilient
   * More performant
   * Make problems tractable

** Argh, distributed systems!

   * Incredibly hard to reason about
   * Huge state space, no repeat scenarios
   * Expensive to operate

** Why distributed systems?

   Because you're /out of options/.

* Questions?
